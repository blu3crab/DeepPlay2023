{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e80b742-0c2e-46d2-99e5-61f2982571e4",
      "metadata": {
        "id": "7e80b742-0c2e-46d2-99e5-61f2982571e4"
      },
      "source": [
        "# Lesson 1: Getting Started with PaLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13b4dd3-3436-43f8-811c-262ed83d7767",
      "metadata": {
        "id": "b13b4dd3-3436-43f8-811c-262ed83d7767"
      },
      "source": [
        "#### Setup\n",
        "Set the MakerSuite API key with the provided helper function."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2n2eR7fS-fWO"
      },
      "id": "2n2eR7fS-fWO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "datetime_object = datetime.datetime(2018, 8, 15, 19, 37, 00, tzinfo=datetime.timezone(datetime.timedelta(hours=-4)))\n",
        "utc_datetime_object = datetime_object.astimezone(datetime.timezone.utc)\n",
        "print(utc_datetime_object)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtRdAnX_6z0q",
        "outputId": "32642aa1-4030-4d09-adbe-295ca06453c1"
      },
      "id": "OtRdAnX_6z0q",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018-08-15 23:37:00+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def to_epoch_time(dt_str):\n",
        "  \"\"\"Converts a datetime string to epoch time.\n",
        "\n",
        "  Args:\n",
        "    dt_str: A datetime string in the format YYYY-MM-DDTHH:MM:SS-TZ.\n",
        "\n",
        "  Returns:\n",
        "    An integer representing the epoch time in seconds.\n",
        "  \"\"\"\n",
        "\n",
        "  dt = datetime.datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%S-%f\")\n",
        "  return int(dt.timestamp())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(to_epoch_time(\"2018-08-15T19:37:00-0400\"))\n",
        "  # 1534400220"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--V5n4647Hqh",
        "outputId": "cce8b7b9-18ab-413a-d25a-a989be072d5b"
      },
      "id": "--V5n4647Hqh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1534361820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def convert_to_minutes(date_string):\n",
        "  \"\"\"Converts a date string to minutes past 2018-01-01.\n",
        "\n",
        "  Args:\n",
        "    date_string: A string in the format YYYY-MM-DDTHH:MM:SS-TZ.\n",
        "\n",
        "  Returns:\n",
        "    An integer representing the number of minutes since 2018-01-01.\n",
        "  \"\"\"\n",
        "\n",
        "  date_time = datetime.datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%S-%f\")\n",
        "  return (date_time - datetime.datetime(2018, 1, 1)).total_seconds() // 60\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  date_string = \"2018-08-15T19:37:00-0400\"\n",
        "  print(convert_to_minutes(date_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa_MofTo8Elm",
        "outputId": "47dd5b92-b457-42fc-e892-d2aedc68ddc1"
      },
      "id": "Oa_MofTo8Elm",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "326617.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2db7275e-7ba3-482c-90a5-8d470dcca05c",
      "metadata": {
        "id": "2db7275e-7ba3-482c-90a5-8d470dcca05c"
      },
      "outputs": [],
      "source": [
        "#from utils import get_api_key\n",
        "def get_api_key():\n",
        "  return \"AIzaSyCZ2070wpJk7csUL_I6AtBLsY341h0rO2I\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -H 'Content-Type: application/json' -d '{ \"prompt\": { \"text\": \"Write a story about a magic backpack\"} }'  \"https://generativelanguage.googleapis.com/v1beta3/models/text-bison-001:generateText?key=AIzaSyCZ2070wpJk7csUL_I6AtBLsY341h0rO2I\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWqvqBJkl4lS",
        "outputId": "57011a45-019a-4549-dfba-6277064941f3"
      },
      "id": "WWqvqBJkl4lS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"candidates\": [\n",
            "    {\n",
            "      \"output\": \"Once upon a time, there was a young boy named Jack who lived in a small village. Jack was a kind and hardworking boy, but he was also very poor. One day, Jack was walking in the forest when he came across a strange old man. The old man was sitting on a rock, and he was crying.\\n\\n\\\"What's wrong?\\\" Jack asked.\\n\\n\\\"I've lost my magic backpack,\\\" the old man said. \\\"It's the only thing I have left in the world.\\\"\\n\\nJack felt sorry for the old man, so he offered to help him find his backpack. The old man gave Jack a few clues, and Jack was able to find the backpack.\\n\\nWhen Jack opened the backpack, he was amazed to see that it was filled with gold coins. He took the backpack to the old man, who was overjoyed to have it back.\\n\\n\\\"Thank you, Jack,\\\" the old man said. \\\"You've saved my life.\\\"\\n\\nJack was happy to have helped the old man, and he was even happier to have the magic backpack. He used the gold coins to buy a new house for his family, and he lived happily ever after.\\n\\nJack soon discovered that the magic backpack had many other uses besides carrying gold coins. It could also hold anything Jack wanted, from food to clothes to tools. Jack used the backpack to help his family and friends, and he soon became known throughout the land as a kind and generous boy.\\n\\nOne day, Jack was walking through the forest when he came across a group of bandits. The bandits were about to attack a traveler when Jack stepped in front of them.\\n\\n\\\"Leave him alone,\\\" Jack said.\\n\\nThe bandits laughed. \\\"What are you going to do about it, boy?\\\" one of them asked.\\n\\nJack opened his magic backpack and pulled out a sword. The bandits were so surprised that they dropped their weapons and ran away.\\n\\nThe traveler thanked Jack for saving him, and Jack continued on his way. He soon realized that the magic backpack could be used to help people in need.\\n\\nJack used the magic backpack to help people all over the land. He helped farmers grow crops, he helped builders build houses, and he even helped the king win a war. Jack became known as a hero, and he was loved by everyone who knew him.\\n\\nOne day, Jack decided to use the magic backpack to find his long-lost parents. He opened the backpack and stepped inside, and he found himself in a strange land. Jack traveled through the land for many days, and finally he came to a village.\\n\\nJack went to the village elders and asked them if they knew where his parents were. The elders told Jack that his parents had died many years ago, but they gave him a clue about his father's past.\\n\\nJack followed the clue, and he eventually found his father's grave. Jack was sad to learn that his father was dead, but he was also glad to have found his grave. Jack said a prayer for his father, and then he turned and walked away.\\n\\nJack continued to use the magic backpack to help people in need. He traveled all over the world, and he made many friends. Jack lived a long and happy life, and he was always grateful for the magic backpack.\",\n",
            "      \"safetyRatings\": [\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_DEROGATORY\",\n",
            "          \"probability\": \"NEGLIGIBLE\"\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_TOXICITY\",\n",
            "          \"probability\": \"NEGLIGIBLE\"\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_VIOLENCE\",\n",
            "          \"probability\": \"NEGLIGIBLE\"\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_SEXUAL\",\n",
            "          \"probability\": \"NEGLIGIBLE\"\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_MEDICAL\",\n",
            "          \"probability\": \"NEGLIGIBLE\"\n",
            "        },\n",
            "        {\n",
            "          \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
            "          \"probability\": \"LOW\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8be2f53-efa5-495f-808e-1e3189f0b73d",
      "metadata": {
        "id": "a8be2f53-efa5-495f-808e-1e3189f0b73d"
      },
      "source": [
        "In this classroom, we've installed the relevant libraries for you.\n",
        "\n",
        "If you wanted to use the PaLM API on your own machine, you would first install the library:\n",
        "```Python\n",
        "!pip install -q google.generativeai\n",
        "```\n",
        "The optional flag `-q` installs \"quietly\" without printing out details of the installation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google.generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIbK1mZhi4K-",
        "outputId": "79a6021b-05fa-4f31-8a4e-9d1c47f8f760"
      },
      "id": "MIbK1mZhi4K-",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m112.6/133.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/267.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.9/267.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "89a9a4b3-b338-4ed8-ac7b-a08143da5b63",
      "metadata": {
        "id": "89a9a4b3-b338-4ed8-ac7b-a08143da5b63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as palm\n",
        "from google.api_core import client_options as client_options_lib\n",
        "\n",
        "palm.configure(\n",
        "    api_key=get_api_key(),\n",
        "    transport=\"rest\",\n",
        "    client_options=client_options_lib.ClientOptions(\n",
        "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9648b897-5ad4-4caa-808d-97528c2fcf39",
      "metadata": {
        "id": "9648b897-5ad4-4caa-808d-97528c2fcf39"
      },
      "source": [
        "### Explore the available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "77038a39-427c-4d1f-bc7e-e0692e8f6869",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77038a39-427c-4d1f-bc7e-e0692e8f6869",
        "outputId": "613d6ce1-44ac-4644-d852-a7f1cf50c81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: models/chat-bison-001\n",
            "description: Chat-optimized generative language model.\n",
            "generation methods:['generateMessage', 'countMessageTokens']\n",
            "\n",
            "name: models/text-bison-001\n",
            "description: Model targeted for text generation.\n",
            "generation methods:['generateText', 'countTextTokens', 'createTunedTextModel']\n",
            "\n",
            "name: models/embedding-gecko-001\n",
            "description: Obtain a distributed representation of a text.\n",
            "generation methods:['embedText']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for m in palm.list_models():\n",
        "    print(f\"name: {m.name}\")\n",
        "    print(f\"description: {m.description}\")\n",
        "    print(f\"generation methods:{m.supported_generation_methods}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8540099-fad0-4954-83a7-c2fba3f6d972",
      "metadata": {
        "id": "e8540099-fad0-4954-83a7-c2fba3f6d972"
      },
      "source": [
        "#### Filter models by their supported generation methods\n",
        "- `generateText` is currently recommended for coding-related prompts.\n",
        "- `generateMessage` is optimized for multi-turn chats (dialogues) with an LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48e26d6a-02b9-4838-a0e6-d2e6a3ae042e",
      "metadata": {
        "id": "48e26d6a-02b9-4838-a0e6-d2e6a3ae042e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387957af-b78d-403b-a905-29b7ec2b533c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Text Bison',\n",
              "       description='Model targeted for text generation.',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "models = [m for m in palm.list_models()\n",
        "          if 'generateText'\n",
        "          in m.supported_generation_methods]\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4eb4fc7d-2a1a-43bc-9810-25e4db3b7cb7",
      "metadata": {
        "id": "4eb4fc7d-2a1a-43bc-9810-25e4db3b7cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188878eb-b4b8-455a-ae96-ec1ae050643d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(name='models/text-bison-001',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Text Bison',\n",
              "      description='Model targeted for text generation.',\n",
              "      input_token_limit=8196,\n",
              "      output_token_limit=1024,\n",
              "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "      temperature=0.7,\n",
              "      top_p=0.95,\n",
              "      top_k=40)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_bison = models[0]\n",
        "model_bison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18a24bff-ebe0-4fd3-93f6-c2aeef4d44f7",
      "metadata": {
        "id": "18a24bff-ebe0-4fd3-93f6-c2aeef4d44f7"
      },
      "source": [
        "#### helper function to generate text\n",
        "\n",
        "- The `@retry` decorator helps you to retry the API call if it fails.\n",
        "- We set the temperature to 0.0 so that the model returns the same output (completion) if given the same input (the prompt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a93ff932-2a86-485a-adad-7f66e285aaf4",
      "metadata": {
        "id": "a93ff932-2a86-485a-adad-7f66e285aaf4"
      },
      "outputs": [],
      "source": [
        "from google.api_core import retry\n",
        "@retry.Retry()\n",
        "def generate_text(prompt,\n",
        "                  model=model_bison,\n",
        "                  temperature=0.0):\n",
        "    return palm.generate_text(prompt=prompt,\n",
        "                              model=model,\n",
        "                              temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa756beb-7e70-4575-a27e-82b733b3d3b0",
      "metadata": {
        "id": "aa756beb-7e70-4575-a27e-82b733b3d3b0"
      },
      "source": [
        "#### Ask the LLM how to write some code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "04080420-acd1-43a8-92bc-7d4c407a0154",
      "metadata": {
        "id": "04080420-acd1-43a8-92bc-7d4c407a0154"
      },
      "outputs": [],
      "source": [
        "prompt = \"Show me how to iterate across a list in Python.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "32a354db-cb9b-4353-b777-4980256f4686",
      "metadata": {
        "id": "32a354db-cb9b-4353-b777-4980256f4686"
      },
      "outputs": [],
      "source": [
        "completion = generate_text(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ef2b1688-2eb7-465c-81cd-555d5b0a5a70",
      "metadata": {
        "id": "ef2b1688-2eb7-465c-81cd-555d5b0a5a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83d4f73-9a70-435b-cf7e-1a9737735108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To iterate across a list in Python, you can use the `for` loop. The syntax is as follows:\n",
            "\n",
            "```python\n",
            "for item in list:\n",
            "  # do something with item\n",
            "```\n",
            "\n",
            "For example, the following code prints each item in the list `my_list`:\n",
            "\n",
            "```python\n",
            "my_list = [\"a\", \"b\", \"c\"]\n",
            "\n",
            "for item in my_list:\n",
            "  print(item)\n",
            "```\n",
            "\n",
            "Output:\n",
            "\n",
            "```\n",
            "a\n",
            "b\n",
            "c\n",
            "```\n",
            "\n",
            "You can also use the `enumerate()` function to iterate over a list and get the index of each item. The syntax is as follows:\n",
            "\n",
            "```python\n",
            "for index, item in enumerate(list):\n",
            "  # do something with index and item\n",
            "```\n",
            "\n",
            "For example, the following code prints the index and value of each item in the list `my_list`:\n",
            "\n",
            "```python\n",
            "my_list = [\"a\", \"b\", \"c\"]\n",
            "\n",
            "for index, item in enumerate(my_list):\n",
            "  print(index, item)\n",
            "```\n",
            "\n",
            "Output:\n",
            "\n",
            "```\n",
            "0 a\n",
            "1 b\n",
            "2 c\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(completion.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1400bcb5-bfe8-4192-809d-d95b21bf8422",
      "metadata": {
        "id": "1400bcb5-bfe8-4192-809d-d95b21bf8422"
      },
      "source": [
        "- Tip: The words \"show me\" tends to encourage the PaLM LLM to give more details and explanations compared to if you were to ask \"write code to ...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b813473-15de-4672-9097-57a3d04219d6",
      "metadata": {
        "id": "6b813473-15de-4672-9097-57a3d04219d6"
      },
      "outputs": [],
      "source": [
        "prompt = \"write code to iterate across a list in Python\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3557557d-2b86-4755-a44f-8846e0035d3a",
      "metadata": {
        "id": "3557557d-2b86-4755-a44f-8846e0035d3a"
      },
      "outputs": [],
      "source": [
        "completion = generate_text(prompt)\n",
        "print(completion.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26114873-bb3c-4253-a679-4dda28af561c",
      "metadata": {
        "id": "26114873-bb3c-4253-a679-4dda28af561c"
      },
      "source": [
        "#### Try out the code\n",
        "- Try copy-pasting some of the generated code and running it in the notebook.\n",
        "- Remember to test out the LLM-generated code and debug it make sure it works as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e2e76677-1b90-4ce4-a3b4-aae857e870f6",
      "metadata": {
        "id": "e2e76677-1b90-4ce4-a3b4-aae857e870f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a270613b-2503-4b70-8d3b-2699dbfe8983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten\n",
            "\n",
            "# Define the model\n",
            "model = tf.keras.Sequential()\n",
            "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(2, 100)))\n",
            "model.add(MaxPooling1D(pool_size=2))\n",
            "model.add(Flatten())\n",
            "model.add(Dense(128, activation='relu'))\n",
            "model.add(Dense(4, activation='softmax'))\n",
            "\n",
            "# Compile the model\n",
            "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
            "\n",
            "# Train the model\n",
            "model.fit(X_train, y_train, epochs=10)\n",
            "\n",
            "# Evaluate the model\n",
            "model.evaluate(X_test, y_test)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# paste the LLM's code here\n",
        "# Modify the prompt with your own question\n",
        "prompt = \"show tensorflow cnn python code for time series with 2 metrics & 4 labels\"\n",
        "\n",
        "completion = generate_text(prompt)\n",
        "print(completion.result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c649daa-781c-4c69-ac1b-d100e9747190",
      "metadata": {
        "id": "3c649daa-781c-4c69-ac1b-d100e9747190"
      },
      "source": [
        "#### Try asking your own coding question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69b4929-ec4f-495c-a773-a92ce2c9b36c",
      "metadata": {
        "id": "c69b4929-ec4f-495c-a773-a92ce2c9b36c"
      },
      "outputs": [],
      "source": [
        "# Modify the prompt with your own question\n",
        "prompt = \"Show me how to [...]\"\n",
        "\n",
        "completion = generate_text(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c759d6a4-ed38-43fd-a588-1d62308a8746",
      "metadata": {
        "id": "c759d6a4-ed38-43fd-a588-1d62308a8746"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6a2b9e7a-4911-476d-9141-010224682d17",
      "metadata": {
        "id": "6a2b9e7a-4911-476d-9141-010224682d17"
      },
      "source": [
        "#### Note about the API key\n",
        "We've provided an API key for this classroom.  If you would like your own API key for your own projects, you can get one at [developers.generativeai.google](https://developers.generativeai.google/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}