{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6S2HVAkSt0p"
      },
      "source": [
        "# Week 2 Assignment: CIFAR-10 Autoencoder\n",
        "\n",
        "For this week, you will create a convolutional autoencoder for the [CIFAR10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset. You are free to choose the architecture of your autoencoder provided that the output image has the same dimensions as the input image.\n",
        "\n",
        "After training, your model should meet loss and accuracy requirements when evaluated with the test dataset. You will then download the model and upload it in the classroom for grading.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4iPr2jyisR"
      },
      "source": [
        "***Important:*** *This colab notebook has read-only access so you won't be able to save your changes. If you want to save your work periodically, please click `File -> Save a Copy in Drive` to create a copy in your account, then work from there.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1mzy2J8_nc1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wPuX3RKxl8l5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c15047-0a55-45ca-a902-cc1d270fde43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install packages for compatibility with the autograder\n",
        "!pip install tensorflow==2.8.0 --quiet\n",
        "!pip install keras==2.8.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3EXwoz-KHtWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cfb540-5a4b-44de-cf62-fde46735d322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf. __version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "obuhO5preT66",
        "outputId": "e459c708-6c6a-4027-fb56-17ef3c1fee05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Gs6Lyc_pd0"
      },
      "source": [
        "## Load and prepare the dataset\n",
        "\n",
        "The [CIFAR 10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset already has train and test splits and you can use those in this exercise. Here are the general steps:\n",
        "\n",
        "* Load the train/test split from TFDS. Set `as_supervised` to `True` so it will be convenient to use the preprocessing function we provided.\n",
        "* Normalize the pixel values to the range [0,1], then return `image, image` pairs for training instead of `image, label`. This is because you will check if the output image is successfully regenerated after going through your autoencoder.\n",
        "* Shuffle and batch the train set. Batch the test set (no need to shuffle).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "outputs": [],
      "source": [
        "# preprocessing function\n",
        "def map_image(image, label):\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "\n",
        "  return image, image # dataset label is not used. replaced with the same image input.\n",
        "\n",
        "# parameters\n",
        "#BATCH_SIZE = 128\n",
        "BATCH_SIZE = 64\n",
        "#SHUFFLE_BUFFER_SIZE = 1024\n",
        "SHUFFLE_BUFFER_SIZE = 512\n",
        "\n",
        "\n",
        "### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "\n",
        "# use tfds.load() to fetch the 'train' split of CIFAR-10\n",
        "#train_dataset = None\n",
        "train_dataset = tfds.load('cifar10', as_supervised=True, split=\"train\")\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "#train_dataset = None\n",
        "train_dataset = train_dataset.map(map_image)\n",
        "\n",
        "# shuffle and batch the dataset\n",
        "#train_dataset = None\n",
        "#train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# use tfds.load() to fetch the 'test' split of CIFAR-10\n",
        "#test_dataset = None\n",
        "test_dataset = tfds.load('cifar10', as_supervised=True, split=\"test\")\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "#test_dataset = None\n",
        "test_dataset = test_dataset.map(map_image)\n",
        "\n",
        "# batch the dataset\n",
        "#test_dataset = None\n",
        "#test_dataset = test_dataset.batch(BATCH_SIZE).repeat()\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyOgGJs_t98"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Create the autoencoder model. As shown in the lectures, you will want to downsample the image in the encoder layers then upsample it in the decoder path. Note that the output layer should be the same dimensions as the original image. Your input images will have the shape `(32, 32, 3)`. If you deviate from this, your model may not be recognized by the grader and may fail.\n",
        "\n",
        "We included a few hints to use the Sequential API below but feel free to remove it and use the Functional API just like in the ungraded labs if you're more comfortable with it. Another reason to use the latter is if you want to visualize the encoder output. As shown in the ungraded labs, it will be easier to indicate multiple outputs with the Functional API. That is not required for this assignment though so you can just stack layers sequentially if you want a simpler solution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(inputs):\n",
        "  '''Defines the encoder with two Conv2D and max pooling layers.'''\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "  max_pool_1 = tf.keras.layers.BatchNormalization()(conv_1)\n",
        "  #conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  # max_pool_1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_1)\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(max_pool_1)\n",
        "  max_pool_2 = tf.keras.layers.BatchNormalization()(conv_2)\n",
        "  #conv_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(max_pool_1)\n",
        "  # max_pool_2 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv_2)\n",
        "\n",
        "  return max_pool_2"
      ],
      "metadata": {
        "id": "ejpYjaOmh7Qi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bottle_neck(inputs):\n",
        "  '''Defines the bottleneck.'''\n",
        "  bottle_neck = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  encoder_visualization = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(bottle_neck)\n",
        "\n",
        "  return bottle_neck, encoder_visualization"
      ],
      "metadata": {
        "id": "ac88mlhhh-IO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(inputs):\n",
        "  '''Defines the decoder path to upsample back to the original image size.'''\n",
        "  #conv_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
        "  conv_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "  up_sample_1 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_1)\n",
        "\n",
        "  #conv_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(up_sample_1)\n",
        "  conv_2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(up_sample_1)\n",
        "  up_sample_2 = tf.keras.layers.UpSampling2D(size=(2,2))(conv_2)\n",
        "\n",
        "  #conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=(3,3), activation='sigmoid', padding='same')(up_sample_2)\n",
        "  conv_3 = tf.keras.layers.Conv2D(filters=1, kernel_size=3, activation='sigmoid', padding='same')(up_sample_2)\n",
        "\n",
        "  return conv_3"
      ],
      "metadata": {
        "id": "M2CwEvrXh-iK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional_auto_encoder():\n",
        "  '''Builds the entire autoencoder model.'''\n",
        "  inputs = tf.keras.layers.Input(shape=(32, 32, 3,))\n",
        "  encoder_output = encoder(inputs)\n",
        "  bottleneck_output, encoder_visualization = bottle_neck(encoder_output)\n",
        "  decoder_output = decoder(bottleneck_output)\n",
        "\n",
        "  model = tf.keras.Model(inputs =inputs, outputs=decoder_output)\n",
        "  encoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_visualization)\n",
        "  return model, encoder_model"
      ],
      "metadata": {
        "id": "S2scc5vAiRq4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wr-Bok3lRgA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04d4dce-2e38-4926-a1b6-d459521e3af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_78 (Conv2D)          (None, 32, 32, 256)       73984     \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 32, 32, 32)        73760     \n",
            "                                                                 \n",
            " up_sampling2d_24 (UpSamplin  (None, 64, 64, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_81 (Conv2D)          (None, 64, 64, 32)        9248      \n",
            "                                                                 \n",
            " up_sampling2d_25 (UpSamplin  (None, 128, 128, 32)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 128, 128, 1)       289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 167,681\n",
            "Trainable params: 167,553\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# suggested layers to use. feel free to add or remove as you see fit.\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "\n",
        "model, convolutional_encoder_model = convolutional_auto_encoder()\n",
        "\n",
        "# use the Sequential API (you can remove if you want to use the Functional API)\n",
        "# model = Sequential()\n",
        "\n",
        "### START CODE HERE ###\n",
        "# use `model.add()` to add layers (if using the Sequential API)\n",
        "#None\n",
        "\n",
        "# inputs = tf.keras.layers.Input(shape = (32, 32, 3))\n",
        "# model.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(128, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(256, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(Conv2D(3, (3, 3), activation = 'relu', padding = 'same'))\n",
        "\n",
        "# model.add(Conv2D(128, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.UpSampling2D(size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.UpSampling2D(size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same'))\n",
        "# model.add(tf.keras.layers.UpSampling2D(size = (2, 2)))\n",
        "\n",
        "# model.add(Conv2D(3, (3, 3), activation = 'sigmoid', padding = 'same'))\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRWTAijKEVUC"
      },
      "source": [
        "## Configure training parameters\n",
        "\n",
        "We have already provided the optimizer, metrics, and loss in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iHIeD9eDETSk"
      },
      "outputs": [],
      "source": [
        "# Please do not change the model.compile() parameters\n",
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQPhm1W_8dC"
      },
      "source": [
        "## Training\n",
        "\n",
        "You can now use [model.fit()](https://keras.io/api/models/model_training_apis/#fit-method) to train your model. You will pass in the `train_dataset` and you are free to configure the other parameters. As with any training, you should see the loss generally going down and the accuracy going up with each epoch. If not, please revisit the previous sections to find possible bugs.\n",
        "\n",
        "*Note: If you get a `dataset length is infinite` error. Please check how you defined `train_dataset`. You might have included a [method that repeats the dataset indefinitely](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat).*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len(train_dataset)->{len(train_dataset)}\")\n",
        "print(f\"len(test_dataset)->{len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07XzZceFuk7R",
        "outputId": "64eb4b1c-d91f-4468-8b29-2ae00f500a1e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_dataset)->782\n",
            "len(test_dataset)->157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AMBimOnsRvg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "outputId": "9a25d5bb-b9a3-46ba-c9a9-47e3869a1940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-610a077bb459>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m### START CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#model.fit(None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mconv_model_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1329, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 128 and 32 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_6/conv2d_82/Sigmoid, IteratorGetNext:1)' with input shapes: [?,128,128,1], [?,32,32,3].\n"
          ]
        }
      ],
      "source": [
        "# parameters (feel free to change this)\n",
        "train_steps = len(train_dataset) // BATCH_SIZE\n",
        "valid_steps = len(test_dataset) // BATCH_SIZE\n",
        "epoch_count = 64  #40\n",
        "\n",
        "### START CODE HERE ###\n",
        "#model.fit(None)\n",
        "conv_model_history = model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, validation_steps=valid_steps, epochs=epoch_count)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT2l1c-SAaF4"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "You can use this code to test your model locally before uploading to the grader. To pass, your model needs to satisfy these two requirements:\n",
        "\n",
        "* loss must be less than 0.01\n",
        "* accuracy must be greater than 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vFncgqahSQhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a03948-241a-4fba-a6d9-17001b96b947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0094 - accuracy: 0.0152\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(test_dataset, steps=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_one_row(disp_images, offset, shape=(32, 32)):\n",
        "  '''Display sample outputs in one row.'''\n",
        "  for idx, test_image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    test_image = np.reshape(test_image, shape)\n",
        "    plt.imshow(test_image, cmap='gray')\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
        "  '''Displays the input, encoded, and decoded output values.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(32,32,))\n",
        "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
        "  display_one_row(disp_predicted, 20, shape=(32,32,))"
      ],
      "metadata": {
        "id": "j8ho2x2PtU19"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# take 1 batch of the dataset\n",
        "test_dataset = test_dataset.take(1)\n",
        "\n",
        "# take the input images and put them in a list\n",
        "output_samples = []\n",
        "for input_image, image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "# pick 10 indices\n",
        "idxs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "\n",
        "# prepare test samples as a batch of 10 images\n",
        "conv_output_samples = np.array(output_samples[idxs])\n",
        "conv_output_samples = np.reshape(conv_output_samples, (10, 32, 32, 3))\n",
        "\n",
        "# get the encoder ouput\n",
        "encoded = convolutional_encoder_model.predict(conv_output_samples)\n",
        "\n",
        "# get a prediction for some values in the dataset\n",
        "predicted = model.predict(conv_output_samples)\n",
        "\n",
        "# display the samples, encodings and decoded values!\n",
        "display_results(conv_output_samples, encoded, predicted, enc_shape=(7,7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ABU7bcDitdOj",
        "outputId": "512d61ba-3dd1-4257-8369-25fbb1005a69"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-53bd46500386>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# display the samples, encodings and decoded values!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdisplay_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_output_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d771f7f40ff0>\u001b[0m in \u001b[0;36mdisplay_results\u001b[0;34m(disp_input_images, disp_encoded, disp_predicted, enc_shape)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;34m'''Displays the input, encoded, and decoded output values.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mdisplay_one_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp_input_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mdisplay_one_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mdisplay_one_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d771f7f40ff0>\u001b[0m in \u001b[0;36mdisplay_one_row\u001b[0;34m(disp_images, offset, shape)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (32,32)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHYAAACFCAYAAACOoQymAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB90lEQVR4nO3WQWrDMBBA0ZHJVs7eRPc/WEAHsPaerpKdG7d1aPv5bythBj6WVDIzQzjTbw+g9zAslGGhDAtlWCjDQhkWyrBQlyObtm2L3nvUWqOU8u6Z9InMjDFGLMsS07T/Xx4K23uP1tppw+nn7vd73G633fVDYWutz4/N83zOZPqWdV2jtfZssudQ2MfxO8+zYf+IV1eijycow0IZFsqwUIaFMiyUYaEMC2VYKMNCGRbKsFCGhTIslGGhDAtlWCjDQhkWyrBQhoUyLJRhoQwLZVgow0IZFsqwUIaFMiyUYaEMC2VYKMNCGRbKsFCGhTIslGGhDAtlWCjDQhkWyrBQhoUyLJRhoQwLZVgow0IZFsqwUIaFMiyUYaEMC2VYKMNCGRbKsFCGhTIslGGhDAtlWCjDQhkWyrBQhoUyLJRhoQwLZVgow0IZFsqwUIaFMiyUYaEMC2VYKMNCGRbKsFCGhTIslGGhDAtlWCjDQhkWyrBQhoUyLJRhoQwLZVgow0IZFsqwUIaFMiyUYaEMC2VYKMNCGRbKsFCGhboc2ZSZERGxrutbh9FrjwaPJnsOhR1jREREa+2HY+ksY4y4Xq+76yVfpY+Ibdui9x611iilnDqgviYzY4wRy7LENO3fpIfC6v/x8QRlWCjDQhkWyrBQhoUyLNQHYWxDkj1kvAgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di6VOHGwIsVM"
      },
      "source": [
        "If you did some visualization like in the ungraded labs, then you might see something like the gallery below. This part is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpI4skkIA5L"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=12Fy-guiP-3tTPfc9IV2nHhqLvs7LwRo6\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaRSkQPNAPT0"
      },
      "source": [
        "## Save your model\n",
        "\n",
        "Once you are satisfied with the results, you can now save your model. Please download it from the Files window on the left and go back to the Submission portal in Coursera for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLFpLP-c7rDR"
      },
      "outputs": [],
      "source": [
        "model.save('mymodel.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QArMiXJTDxDe"
      },
      "source": [
        "**Congratulations on completing this week's assignment!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}